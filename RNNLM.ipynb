{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import nltk\n",
    "from utils import softmax\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling\n",
    "\n",
    "Formulas, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data and Preprocessing\n",
    "\n",
    "To train our language model we need text to learn from. Fortunately we don't need any labels because all we want to do is predict the next word given a sequence of previous words.\n",
    "\n",
    "I opted to use 15,000 reddit commits that I downloaded from a [dataset available on Google's BigQuery](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08). So our model will be talking like reddit commenters!\n",
    "\n",
    "Before we can use the data to train our Neural Network there are a couple of things we must do:\n",
    "\n",
    "#### 1. Tokenizing Text\n",
    "\n",
    "We want to predict on a per-word basis, so we must *tokenize* the sentences into words. You could just split each of the comments by spaces, but that wouldn't properly tokenize punctuation. The sentence \"He left!\"  should be 3 tokens, \"He\", \"left\" and \"!\". That's why we use [NLTK's](http://www.nltk.org/) `word_tokenize` method, which does that for us.\n",
    "\n",
    "#### 2. Remove Infrequent words\n",
    "\n",
    "Get rid of infrequent words. The reason for this is two-fold. First, having a huge vocabulary will make our model very slow to train (we'll talk about why that is later). Secondly, because we don't have a lot of contextual examples for these infrequent words we probably wouldn't be able to learn how to use them correctly anyway. That's quite similar to humans learn. To really understand the how to use a word appropriately you want to see it in different contexts. In the code below we limit our vocabulary to the `vocabulary_size` most common words. All words that we don't include in our vocabulary will be replaced by \"UNKNOWN_TOKEN\" in a sentence. For example, if we don't include the word \"nonlinearities\" in our vocabulary the sentence \"nonlineraties are important in Neural Networks\" would become \"UNKNOWN_TOKEN are important in Neural Networks\". The \"UNKNOWN_TOKEN\" word is always includes in our vocabulary and we predict it just like any other word.\n",
    "\n",
    "#### 3. Prepend special start and end tokens\n",
    "\n",
    "We prepend a special start token `SENTENCE_START` to each sentence, and similarly we append a special `SENTENCE_END` token to each sentence. This will help us when generating new sentences later on, because we can ask, given that the first word is \"SENTENCE_END\", what is the likely next word? And similarly, if our model predicts an \"SENTENCE_END\" token we know we should stop.\n",
    "\n",
    "#### 4. Create word mappings\n",
    "\n",
    "We create a mapping between words and indices, `index_to_word`, and `word_to_index`. For example, \"network\" may be index `301`.\n",
    "\n",
    "#### 5. Build training data\n",
    "\n",
    "For example, \"he went home.\" could be `[999, 179, 341, 416, 0]`, where 999 maps to `SENTENCE_START` and 1000 maps to `SENTENCE_END` This is one training example, x. The corresponding y vector would be `[179, 341, 416, 9, 1000]`. It just the x vector shifted by one position and the last element is the `SENTENCE_END` token. Also, we don't actually want to represent the words in our x vectors as integers. Rather, we need to them to be one-hot vectors. A one-hot vector for 179 would be a vector of length `vocabulary_size` with all elements 0 except for the element at position 179, which is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We keep this many words\n",
    "vocabulary_size = 2500\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "with open('data/reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x[0].decode('utf-8').lower(), sentence_end_token) for x in reader]\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = dict(enumerate(x[0] for x in vocab))\n",
    "index_to_word[vocabulary_size-1] = unknown_token\n",
    "word_to_index = inv_map = {v: k for k, v in index_to_word.items()}\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 'SENTENCE_START i think you might be': [13, 4, 73, 8, 189, 26]\n",
      "y: 'i think you might be missing': [4, 73, 8, 189, 26, 995]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example = X_train[8][:6]\n",
    "y_example = y_train[8][:6]\n",
    "print \"X: '%s': %s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
    "print \"y: '%s': %s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the RNN\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review the equations for a RNN:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "s_t &= \\tanh(Ux_t + Ws_{t_1}) \\\\\n",
    "o_t &= \\mathrm{softmax}(Vs_t)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Helper matrix to convert X into a one-hot vector\n",
    "        self.to_onehot = np.eye(self.word_dim)\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.randn(hidden_dim, word_dim) * np.sqrt(2.0/word_dim)\n",
    "        self.V = np.random.randn(word_dim, hidden_dim) * np.sqrt(2.0/hidden_dim)\n",
    "        self.W = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2.0/hidden_dim)\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The number of time steps\n",
    "        x = self.to_onehot[x]\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s. We need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            s[t] = np.tanh(self.U.dot(x[t]) + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        # We accumulate the total loss in L\n",
    "        L = 0\n",
    "        for i in np.arange(len(y)):\n",
    "            x_i, y_i = X[i], y[i]\n",
    "            o, s = self.forward_propagation(x_i)\n",
    "            L += -1 * np.sum(np.log(o[np.arange(len(y_i)), y_i]))\n",
    "        return L\n",
    "    \n",
    "    def calculate_mean_loss(self, X, y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y_i) for y_i in y])\n",
    "        return self.calculate_loss(X,y)/float(num_words)   \n",
    "            \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "                dLdU += np.outer(delta_t, self.to_onehot[x[bptt_step]])\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 50.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 50.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(model, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Overwrite the bptt attribute. We need to backpropagate all the way to get the correct gradient\n",
    "    model.bptt_truncate = 1000\n",
    "    # Calculate the gradients using backprop\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to chec.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(model)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "            \n",
    "            \n",
    "np.random.seed(10)\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 5\n",
    "model = RNNNumpy(grad_check_vocab_size, 10)\n",
    "gradient_check(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_rnn_with_sgd(model, X_train, y_train, learning_rate=0.0005, print_loss_after=1000, nepoch=1, anneal_after=-1, anneal_factor=0.5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        for i in np.arange(len(y_train)):\n",
    "            x_i, y_i = X_train[i], y_train[i]\n",
    "            dLdU, dLdV, dLdW = model.bptt(x_i, y_i)\n",
    "            model.U -= learning_rate * dLdU\n",
    "            model.V -= learning_rate * dLdV\n",
    "            model.W -= learning_rate * dLdW\n",
    "            if (i % print_loss_after == 0):\n",
    "                loss = model.calculate_mean_loss(X_train, y_train)\n",
    "                losses.append(loss)\n",
    "                print \"Loss after epoch=%d i=%d: %f\" % (epoch, i,loss)\n",
    "        # Adjust the learning rate\n",
    "        if(epoch % anneal_after == 0):\n",
    "            learning_rate = learning_rate * anneal_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(10)\n",
    "# model = RNNNumpy(vocabulary_size, 100, bptt_truncate=4)\n",
    "# losses = train_rnn_with_sgd(model, X_train_onehot[:100], y_train[:100], nepoch=100, learning_rate=0.005, anneal_after=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00042649  0.000451    0.00043382  0.00041876  0.00038081  0.00039459\n",
      "  0.00036433  0.00038306  0.00038365  0.00039345]\n"
     ]
    }
   ],
   "source": [
    "# How long would it take to train a model\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size, 100, bptt_truncate=4)\n",
    "# %timeit model.forward_propagation(X_train[3])\n",
    "# %timeit model.bptt(X_train[3], y_train[3])\n",
    "o, s = model.forward_propagation(X_train[3])\n",
    "print o[1,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        U = np.random.randn(hidden_dim, word_dim) * np.sqrt(2.0/word_dim)\n",
    "        V = np.random.randn(word_dim, hidden_dim) * np.sqrt(2.0/hidden_dim)\n",
    "        W = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2.0/hidden_dim)\n",
    "        # Theano: Created shared variables\n",
    "        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        # We store the Theano graph here\n",
    "        self.theano = {}\n",
    "        self.__theano_build__()\n",
    "    \n",
    "    def __theano_build__(self):\n",
    "        U, V, W = self.U, self.V, self.W\n",
    "        x = T.ivector('x')\n",
    "        y = T.ivector('y')\n",
    "        x_onehot = T.extra_ops.to_one_hot(x, self.word_dim)\n",
    "        def forward_prop_step(x_t, s_t_prev, U, V, W):\n",
    "            s_t = T.tanh(U.dot(x_t) + W.dot(s_t_prev))\n",
    "            o_t = T.nnet.softmax(V.dot(s_t))\n",
    "            return [o_t[0], s_t]\n",
    "        [o,s], updates = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=x_onehot,\n",
    "            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],\n",
    "            non_sequences=[U, V, W],\n",
    "            truncate_gradient=self.bptt_truncate,\n",
    "            strict=True)\n",
    "        \n",
    "        prediction = T.argmax(o, axis=1)\n",
    "        o_error = T.sum(T.nnet.categorical_crossentropy(o, y))\n",
    "        \n",
    "        # Gradients\n",
    "        dU = T.grad(o_error, U)\n",
    "        dV = T.grad(o_error, V)\n",
    "        dW = T.grad(o_error, W)\n",
    "        \n",
    "        # Assign functions\n",
    "        self.forward_propagation = theano.function([x], o)\n",
    "        self.predict = theano.function([x], prediction)\n",
    "        self.ce_error = theano.function([x, y], o_error)\n",
    "        self.bptt = theano.function([x, y], [dU, dV, dW])\n",
    "        \n",
    "        # SGD\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        self.sgd_step = theano.function([x,y,learning_rate], [], \n",
    "                      updates=[(self.U, self.U - learning_rate * dU),\n",
    "                              (self.V, self.V - learning_rate * dV),\n",
    "                              (self.W, self.W - learning_rate * dW)])\n",
    "    \n",
    "    def calculate_loss(self, X, Y):\n",
    "        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])\n",
    "    \n",
    "    def calculate_mean_loss(self, X, Y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y) for y in Y])\n",
    "        return self.calculate_loss(X,Y)/float(num_words)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 50.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 50.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "def gradient_check_theano(model, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Overwrite the bptt attribute. We need to backpropagate all the way to get the correct gradient\n",
    "    model.bptt_truncate = 1000\n",
    "    # Calculate the gradients using backprop\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to chec.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter_T = operator.attrgetter(pname)(model)\n",
    "        parameter = parameter_T.get_value()\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            parameter_T.set_value(parameter)\n",
    "            gradplus = model.calculate_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            parameter_T.set_value(parameter)\n",
    "            gradminus = model.calculate_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            parameter[ix] = original_value\n",
    "            parameter_T.set_value(parameter)\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "        \n",
    "np.random.seed(10)\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 5\n",
    "model = RNNTheano(grad_check_vocab_size, 10)\n",
    "gradient_check_theano(model, [0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNTheano(vocabulary_size, 100, bptt_truncate=4)\n",
    "# model.forward_propagation(X_train[3])[1][:10]\n",
    "# model.predict(X_train[3])\n",
    "# model.ce_error(X_train[3], y_train[3])\n",
    "# print X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 4.04 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit model.calculate_mean_loss(X_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_rnn_with_sgd_theano(model, X_train, y_train, learning_rate=0.0005, print_loss_after=5000, nepoch=1, anneal_after=-1, anneal_factor=0.5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        for i in np.arange(len(y_train)):\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            if (num_examples_seen % print_loss_after == 0):\n",
    "                loss = model.calculate_mean_loss(X_train, y_train)\n",
    "                losses.append(loss)\n",
    "                print \"Loss after epoch=%d i=%d: %f\" % (epoch, i,loss)\n",
    "            num_examples_seen += 1\n",
    "        # Adjust the learning rate\n",
    "        if(epoch % anneal_after == 0):\n",
    "            learning_rate = learning_rate * anneal_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNTheano(vocabulary_size, 100, bptt_truncate=4)\n",
    "losses = train_rnn_with_sgd_theano(model, X_train[:5000], y_train[:5000], nepoch=100, learning_rate=0.005, anneal_after=5, print_loss_after=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
