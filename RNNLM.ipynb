{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import nltk\n",
    "from utils import softmax\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Loading and Preparation\n",
    "vocabulary_size = 1000\n",
    "unknown_token = \"<UNKNOWN/>\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "with open('data/reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x[0].decode('utf-8'), sentence_end_token) for x in reader]\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = dict(enumerate(x[0] for x in vocab))\n",
    "index_to_word[vocabulary_size-1] = unknown_token\n",
    "word_to_index = inv_map = {v: k for k, v in index_to_word.items()}\n",
    "\n",
    "# Replace all words not in our vocabulary with [unknown_token]\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 266, 1, 8, 72, 455, 77, 5, 94, 9, 16, 298, 7, 999, 892, 27, 26, 17, 168, 38, 31, 26, 999, 3, 999, 999, 999, 4, 999, 3, 999, 1, 28, 913, 12, 146, 166, 27, 5, 103, 327, 66, 26, 666, 88, 999, 3, 2, 999, 428, 0]\n",
      "SENTENCE_START No , you 're fine - I know it 's kind of <UNKNOWN/> involved but was n't sure if this was <UNKNOWN/> to <UNKNOWN/> <UNKNOWN/> <UNKNOWN/> a <UNKNOWN/> to <UNKNOWN/> , not posted in here before but I could tell there was absolutely no <UNKNOWN/> to the <UNKNOWN/> number .\n",
      "[266, 1, 8, 72, 455, 77, 5, 94, 9, 16, 298, 7, 999, 892, 27, 26, 17, 168, 38, 31, 26, 999, 3, 999, 999, 999, 4, 999, 3, 999, 1, 28, 913, 12, 146, 166, 27, 5, 103, 327, 66, 26, 666, 88, 999, 3, 2, 999, 428, 0, 14]\n",
      "No , you 're fine - I know it 's kind of <UNKNOWN/> involved but was n't sure if this was <UNKNOWN/> to <UNKNOWN/> <UNKNOWN/> <UNKNOWN/> a <UNKNOWN/> to <UNKNOWN/> , not posted in here before but I could tell there was absolutely no <UNKNOWN/> to the <UNKNOWN/> number . SENTENCE_END\n"
     ]
    }
   ],
   "source": [
    "# Creates a one-hot vector\n",
    "def make_onehot(x, vocabulary_size=vocabulary_size):\n",
    "    result = np.zeros((len(x), vocabulary_size))\n",
    "    result[np.arange(len(x)), x] = 1\n",
    "    return result\n",
    "\n",
    "X_train = np.array([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "X_train_onehot = np.array([make_onehot(x) for x in X_train])\n",
    "y_train = np.array([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "print X_train[123]\n",
    "print \" \".join([index_to_word[x] for x in X_train[123]])\n",
    "print y_train[123]\n",
    "print \" \".join([index_to_word[x] for x in y_train[123]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's review the equations for a RNN:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "s_t &= \\tanh(Ux_t + Ws_{t_1}) \\\\\n",
    "o_t &= \\mathrm{softmax}(Vs_t)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.randn(hidden_dim, word_dim) * np.sqrt(2.0/word_dim)\n",
    "        self.V = np.random.randn(word_dim, hidden_dim) * np.sqrt(2.0/hidden_dim)\n",
    "        self.W = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2.0/hidden_dim)\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s. We need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            s[t] = np.tanh(self.U.dot(x[t]) + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        # We accumulate the total loss in L\n",
    "        L = 0\n",
    "        for i in np.arange(len(y)):\n",
    "            x_i, y_i = X[i], y[i]\n",
    "            o, s = self.forward_propagation(x_i)\n",
    "            L += -1 * np.sum(np.log(o[np.arange(len(y_i)), y_i]))\n",
    "        return L\n",
    "    \n",
    "    def calculate_mean_loss(self, X, y):\n",
    "        # Divide calculate_loss by the number of words\n",
    "        num_words = np.sum([len(y_i) for y_i in y])\n",
    "        return self.calculate_loss(X,y)/float(num_words)   \n",
    "            \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        # Perform forward propagation\n",
    "        o, s = self.forward_propagation(x)\n",
    "        # We accumulate the gradients in these variables\n",
    "        dLdU = np.zeros(self.U.shape)\n",
    "        dLdV = np.zeros(self.V.shape)\n",
    "        dLdW = np.zeros(self.W.shape)\n",
    "        delta_o = o\n",
    "        delta_o[np.arange(len(y)), y] -= 1.\n",
    "        # For each output backwards...\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dLdV += np.outer(delta_o[t], s[t].T)\n",
    "            # Initial delta calculation\n",
    "            delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "            # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "                # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "                dLdW += np.outer(delta_t, s[bptt_step-1])\n",
    "                dLdU += np.outer(delta_t, x[bptt_step])\n",
    "                # Update delta for next step\n",
    "                delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "        return [dLdU, dLdV, dLdW]\n",
    "    \n",
    "\n",
    "model = RNNNumpy(2, 10)\n",
    "# model.bptt(np.random.randn(2,2), [0, 0])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 50.\n",
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 50.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(model, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Overwrite the bptt attribute. We need to backpropagate all the way to get the correct gradient\n",
    "    model.bptt_truncate = 1000\n",
    "    # Calculate the gradients using backprop\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to chec.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(model)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return \n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    "            \n",
    "            \n",
    "np.random.seed(10)\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 5\n",
    "model = RNNNumpy(check_vocab_size, 10)\n",
    "gradient_check(model, make_onehot([0,1,2,3], grad_check_vocab_size), [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_rnn_with_sgd(model, X_train, y_train, learning_rate=0.0005, print_loss_after=1000, nepoch=1, anneal_after=-1, anneal_factor=0.5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        for i in np.arange(len(y_train)):\n",
    "            x_i, y_i = X_train[i], y_train[i]\n",
    "            dLdU, dLdV, dLdW = model.bptt(x_i, y_i)\n",
    "            model.U -= learning_rate * dLdU\n",
    "            model.V -= learning_rate * dLdV\n",
    "            model.W -= learning_rate * dLdW\n",
    "            if (i % print_loss_after == 0):\n",
    "                loss = model.calculate_mean_loss(X_train, y_train)\n",
    "                losses.append(loss)\n",
    "                print \"Loss after epoch=%d i=%d: %f\" % (epoch, i,loss)\n",
    "        # Adjust the learning rate\n",
    "        if(epoch % anneal_after == 0):\n",
    "            learning_rate = learning_rate * anneal_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch=0 i=0: 7.271489\n",
      "Loss after epoch=1 i=0: 5.074434\n",
      "Loss after epoch=2 i=0: 4.864124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-df89ca8c145e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt_truncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn_with_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal_after\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-1f681b6260af>\u001b[0m in \u001b[0;36mtrain_rnn_with_sgd\u001b[0;34m(model, X_train, y_train, learning_rate, print_loss_after, nepoch, anneal_after, anneal_factor)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mdLdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdLdU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdLdV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-03b3bf94aa29>\u001b[0m in \u001b[0;36mbptt\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mdLdW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mdLdU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# Update delta for next step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mdelta_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbptt_step\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/wildml/rnn-tutorial-rnnlm/venv/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mouter\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;31m# try to import blas optimized dot if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size, 100, bptt_truncate=4)\n",
    "losses = train_rnn_with_sgd(model, X_train_onehot[:100], y_train[:100], nepoch=100, learning_rate=0.005, anneal_after=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 187 ms per loop\n",
      "Total training time: 31 hours\n"
     ]
    }
   ],
   "source": [
    "# How long would it take to train a model\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size, 100, bptt_truncate=4)\n",
    "x = %timeit -o model.bptt(X_train_onehot[3], y_train[3])\n",
    "print \"Total training time: %d hours\" % (x.best * len(y_train) * 40 / (3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNTheano:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Initialize the parameters\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # TODO: What about the bias!?\n",
    "        U = 0.1 * np.random.randn(hidden_dim, word_dim)\n",
    "        W = 0.1 * np.random.randn(hidden_dim, hidden_dim)\n",
    "        V = 0.1 * np.random.randn(word_dim, hidden_dim)\n",
    "        self.params = {}\n",
    "        self.params['U'] = theano.shared(name='U', value=U.astype(theano.config.floatX))\n",
    "        self.params['V'] = theano.shared(name='V', value=V.astype(theano.config.floatX))\n",
    "        self.params['W'] = theano.shared(name='W', value=W.astype(theano.config.floatX))\n",
    "        self.comp = self.build_computation_graph()\n",
    "    \n",
    "    def build_computation_graph(self):\n",
    "        # Define Variables\n",
    "        U, V, W = self.params['U'], self.params['V'], self.params['W']\n",
    "        x_t = T.vector('x_t', theano.config.floatX)\n",
    "        s_t_prev = T.vector('s_t_prev', theano.config.floatX)\n",
    "        s_initial = T.vector('s_initial', theano.config.floatX)\n",
    "        words = T.matrix('x', theano.config.floatX)\n",
    "        # All the hidden states and outputs\n",
    "        s = T.matrix('s')\n",
    "        o = T.matrix('o')\n",
    "        # A function that returns the hidden states and outputs\n",
    "        def forward_prop_step(x_t, s_t_prev, o_t_prev, U, V, W):\n",
    "            s_t = T.tanh(U * x_t + W * s_t_prev)\n",
    "            o_t = T.nnet.softmax(V * s_t)\n",
    "            return [s_t, o_t]\n",
    "        ([s_vals, o_vals], updates) = theano.scan(\n",
    "            forward_prop_step,\n",
    "            sequences=words,\n",
    "            outputs_info=[s, o],\n",
    "            non_sequences=[U, V, W],\n",
    "            strict=True\n",
    "        )\n",
    "        forward_prop = theano.function([s, o, words], [s_vals, o_vals])\n",
    "        return {\n",
    "            'forward_prop': forward_prop\n",
    "        }\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        s_initial = np.zeros((len(x)+1, self.hidden_dim))\n",
    "        o_initial = np.zeros((len(x)+1, self.word_dim))\n",
    "        s, o = self.comp['forward_prop'](s_initial, o_initial, x)\n",
    "        return [s, o]\n",
    "    \n",
    "    def calculate_loss(self, X, y):\n",
    "        return 0\n",
    "    \n",
    "    def predict(self):\n",
    "        # TODO\n",
    "        return nil\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        # TODO\n",
    "        return nil\n",
    "\n",
    "model = RNNTheano(vocabulary_size, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[0].shape[0] = 50, input[2].shape[0] = 108)\nApply node that caused the error: Elemwise{Composite{tanh((i0 + (i1 * i2)))}}(<TensorType(float64, matrix)>, W_copy, s[t-1])\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{0}.0, Elemwise{mul,no_inplace}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0, W, V)\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix)]\nInputs shapes: [(), (107, 50, 1000), (107, 108, 50), (107, 108, 1000), (50, 50), (1000, 50)]\nInputs strides: [(), (400000, 8000, 8), (43200, 400, 8), (864000, 8000, 8), (400, 8), (400, 8)]\nInputs values: [array(107), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-19f3030869a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-51716aef4dc6>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0ms_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mo_initial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'forward_prop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/wildml/rnn-tutorial-rnnlm/venv/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                     \u001b[0;31m# For the c linker We don't have access from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/wildml/rnn-tutorial-rnnlm/venv/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/wildml/rnn-tutorial-rnnlm/venv/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    670\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    671\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/wildml/rnn-tutorial-rnnlm/venv/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    659\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/dennybritz/.theano/compiledir_Darwin-14.5.0-x86_64-i386-64bit-i386-2.7.10-64/scan_perform/mod.cpp:3605)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/dennybritz/.theano/compiledir_Darwin-14.5.0-x86_64-i386-64bit-i386-2.7.10-64/scan_perform/mod.cpp:3537)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input dimension mis-match. (input[0].shape[0] = 50, input[2].shape[0] = 108)\nApply node that caused the error: Elemwise{Composite{tanh((i0 + (i1 * i2)))}}(<TensorType(float64, matrix)>, W_copy, s[t-1])\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, matrix)]\n\nHINT: Use another linker then the c linker to have the inputs shapes and strides printed.\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.\nApply node that caused the error: forall_inplace,cpu,scan_fn}(Shape_i{0}.0, Elemwise{mul,no_inplace}.0, IncSubtensor{InplaceSet;:int64:}.0, IncSubtensor{InplaceSet;:int64:}.0, W, V)\nInputs types: [TensorType(int64, scalar), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, 3D), TensorType(float64, matrix), TensorType(float64, matrix)]\nInputs shapes: [(), (107, 50, 1000), (107, 108, 50), (107, 108, 1000), (50, 50), (1000, 50)]\nInputs strides: [(), (400000, 8000, 8), (43200, 400, 8), (864000, 8000, 8), (400, 8), (400, 8)]\nInputs values: [array(107), 'not shown', 'not shown', 'not shown', 'not shown', 'not shown']\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "s, o = model.forward_propagation(X_train_onehot[3])\n",
    "print o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
